{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch as tc\n",
    "import torchvision as tv\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "1. define tensor with \"requires_grad=True\"\n",
    "2. write formula\n",
    "3. call .backward()\n",
    "Example: gradiant on $B=x^TAx$, $\\frac{dB}{dx}=(A+A^T)x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1],\n",
      "        [ 2],\n",
      "        [ 3]])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 2,  4,  4],\n",
      "        [ 3,  4,  5]])\n"
     ]
    }
   ],
   "source": [
    "x = tc.tensor([[1],[2],[3]], requires_grad=True)\n",
    "A = tc.tensor([[1,2,3],[2,4,4],[3,4,5]])\n",
    "print(x)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 136]])\n",
      "tensor([[ 28],\n",
      "        [ 44],\n",
      "        [ 52]])\n",
      "tensor([[ 28],\n",
      "        [ 44],\n",
      "        [ 52]])\n"
     ]
    }
   ],
   "source": [
    "B = x.t().mm(A).mm(x)\n",
    "print(B)\n",
    "# x.grad.zero_()\n",
    "B.backward()\n",
    "print(x.grad)\n",
    "actual_grad = (A+A.t()).mm(x)\n",
    "print( actual_grad )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using pytorch\n",
    "What do we do:\n",
    "```python\n",
    "step 0: load data and parameter\n",
    "\n",
    "step 1: def forward(X):\n",
    "            # pass\n",
    "            return y_pred\n",
    "step 2: def loss_fn(y_pred, y_target):\n",
    "            # pass\n",
    "            return loss\n",
    "step 3: def update(loss,W_old)：\n",
    "            # pass:\n",
    "            return W_new\n",
    "```\n",
    "Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tc.rand([100,5])\n",
    "y = tc.empty(100, dtype=tc.long).random_(3)\n",
    "W = tc.rand([5,3],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6077,  0.5065,  0.7217,  0.6019,  0.3107],\n",
      "        [ 0.6881,  0.6758,  0.5778,  0.1460,  0.9403],\n",
      "        [ 0.0885,  0.5016,  0.4405,  0.4557,  0.8329],\n",
      "        [ 0.6146,  0.5280,  0.1502,  0.8507,  0.8534],\n",
      "        [ 0.4453,  0.6853,  0.1063,  0.3200,  0.1786],\n",
      "        [ 0.0724,  0.9152,  0.0275,  0.9943,  0.0547],\n",
      "        [ 0.8005,  0.6575,  0.4772,  0.7157,  0.9138],\n",
      "        [ 0.0318,  0.6873,  0.2280,  0.4942,  0.9803],\n",
      "        [ 0.2753,  0.0258,  0.4273,  0.2155,  0.7636],\n",
      "        [ 0.7210,  0.2274,  0.3189,  0.8020,  0.6872]])\n",
      "tensor([ 2,  2,  0,  1,  2,  2,  1,  0,  1,  2])\n",
      "tensor([[ 0.5767,  0.0003,  0.3764],\n",
      "        [ 0.1501,  0.4021,  0.8631],\n",
      "        [ 0.8687,  0.5672,  0.2082],\n",
      "        [ 0.3000,  0.9565,  0.4771],\n",
      "        [ 0.7156,  0.9555,  0.9857]])\n"
     ]
    }
   ],
   "source": [
    "print(X[:10])\n",
    "print(y[:10])\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__torch.nn__ includes may classes that act like \"functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_loss_func = tc.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use his function to calculate the numerial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_loss = logreg_loss_func(X.mm(W), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call __.backward()__ to see get the gradiant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-02 *\n",
       "       [[ 2.7252,  1.5970, -4.3222],\n",
       "        [ 2.3564,  2.5113, -4.8677],\n",
       "        [ 1.8508,  2.9365, -4.7873],\n",
       "        [-1.7610,  4.5200, -2.7589],\n",
       "        [ 1.0130,  0.0583, -1.0713]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W.grad.zero_()\n",
    "logreg_loss.backward(retain_graph=True)\n",
    "grad = W.grad\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5770,  0.0004,  0.3759],\n",
       "        [ 0.1503,  0.4023,  0.8626],\n",
       "        [ 0.8689,  0.5675,  0.2077],\n",
       "        [ 0.2998,  0.9570,  0.4769],\n",
       "        [ 0.7157,  0.9555,  0.9856]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tc.no_grad():\n",
    "    lr = 0.01\n",
    "    W = W + grad * lr\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we update W, we could see all difficult part are done by pytorch\n",
    "## Encapsulation even more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = tc.randn(N, D_in)\n",
    "y = tc.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_fn = tc.nn.Sequential(\n",
    "    tc.nn.Linear(D_in, H),\n",
    "    tc.nn.ReLU(),\n",
    "    tc.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = tc.nn.MSELoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0670)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = forward_fn(x)\n",
    "loss = loss_fn(y_pred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.067003607749939\n",
      "1 1.0668734312057495\n",
      "2 1.066742181777954\n",
      "3 1.0666124820709229\n",
      "4 1.0664807558059692\n",
      "5 1.0663511753082275\n",
      "6 1.0662205219268799\n",
      "7 1.0660898685455322\n",
      "8 1.065960168838501\n",
      "9 1.0658293962478638\n",
      "10 1.0656994581222534\n",
      "11 1.065568208694458\n",
      "12 1.065439224243164\n",
      "13 1.065308690071106\n",
      "14 1.0651781558990479\n",
      "15 1.0650476217269897\n",
      "16 1.0649176836013794\n",
      "17 1.0647869110107422\n",
      "18 1.0646580457687378\n",
      "19 1.0645275115966797\n",
      "20 1.064396619796753\n",
      "21 1.0642671585083008\n",
      "22 1.064137578010559\n",
      "23 1.0640074014663696\n",
      "24 1.0638773441314697\n",
      "25 1.0637468099594116\n",
      "26 1.06361722946167\n",
      "27 1.0634886026382446\n",
      "28 1.063357949256897\n",
      "29 1.0632283687591553\n",
      "30 1.0630995035171509\n",
      "31 1.0629692077636719\n",
      "32 1.0628385543823242\n",
      "33 1.062709093093872\n",
      "34 1.062578558921814\n",
      "35 1.062449336051941\n",
      "36 1.062320351600647\n",
      "37 1.0621904134750366\n",
      "38 1.0620611906051636\n",
      "39 1.061930775642395\n",
      "40 1.0618013143539429\n",
      "41 1.0616724491119385\n",
      "42 1.0615429878234863\n",
      "43 1.0614131689071655\n",
      "44 1.061283826828003\n",
      "45 1.061154842376709\n",
      "46 1.0610249042510986\n",
      "47 1.0608952045440674\n",
      "48 1.0607659816741943\n",
      "49 1.0606367588043213\n",
      "50 1.0605074167251587\n",
      "51 1.0603779554367065\n",
      "52 1.060248613357544\n",
      "53 1.06011962890625\n",
      "54 1.0599905252456665\n",
      "55 1.0598613023757935\n",
      "56 1.0597318410873413\n",
      "57 1.0596027374267578\n",
      "58 1.0594735145568848\n",
      "59 1.0593440532684326\n",
      "60 1.059215784072876\n",
      "61 1.0590858459472656\n",
      "62 1.0589573383331299\n",
      "63 1.0588282346725464\n",
      "64 1.0586984157562256\n",
      "65 1.0585706233978271\n",
      "66 1.058441400527954\n",
      "67 1.0583126544952393\n",
      "68 1.0581836700439453\n",
      "69 1.0580546855926514\n",
      "70 1.057925820350647\n",
      "71 1.0577971935272217\n",
      "72 1.0576684474945068\n",
      "73 1.0575400590896606\n",
      "74 1.0574105978012085\n",
      "75 1.0572820901870728\n",
      "76 1.0571539402008057\n",
      "77 1.0570248365402222\n",
      "78 1.0568969249725342\n",
      "79 1.0567677021026611\n",
      "80 1.0566396713256836\n",
      "81 1.056511402130127\n",
      "82 1.0563827753067017\n",
      "83 1.0562541484832764\n",
      "84 1.0561264753341675\n",
      "85 1.05599844455719\n",
      "86 1.0558704137802124\n",
      "87 1.0557420253753662\n",
      "88 1.0556138753890991\n",
      "89 1.0554866790771484\n",
      "90 1.0553579330444336\n",
      "91 1.0552303791046143\n",
      "92 1.0551023483276367\n",
      "93 1.0549741983413696\n",
      "94 1.0548462867736816\n",
      "95 1.0547189712524414\n",
      "96 1.0545909404754639\n",
      "97 1.0544629096984863\n",
      "98 1.0543347597122192\n",
      "99 1.0542072057724\n",
      "100 1.054079294204712\n",
      "101 1.0539524555206299\n",
      "102 1.0538244247436523\n",
      "103 1.053696870803833\n",
      "104 1.0535695552825928\n",
      "105 1.0534409284591675\n",
      "106 1.053313970565796\n",
      "107 1.0531861782073975\n",
      "108 1.0530587434768677\n",
      "109 1.0529316663742065\n",
      "110 1.05280339717865\n",
      "111 1.052675724029541\n",
      "112 1.0525490045547485\n",
      "113 1.052420973777771\n",
      "114 1.0522940158843994\n",
      "115 1.0521659851074219\n",
      "116 1.0520399808883667\n",
      "117 1.0519123077392578\n",
      "118 1.051784634590149\n",
      "119 1.05165696144104\n",
      "120 1.0515295267105103\n",
      "121 1.0514024496078491\n",
      "122 1.0512758493423462\n",
      "123 1.051148772239685\n",
      "124 1.0510202646255493\n",
      "125 1.0508949756622314\n",
      "126 1.0507670640945435\n",
      "127 1.0506398677825928\n",
      "128 1.0505130290985107\n",
      "129 1.0503857135772705\n",
      "130 1.0502591133117676\n",
      "131 1.0501317977905273\n",
      "132 1.0500051975250244\n",
      "133 1.0498780012130737\n",
      "134 1.04975163936615\n",
      "135 1.0496242046356201\n",
      "136 1.0494978427886963\n",
      "137 1.0493706464767456\n",
      "138 1.04924476146698\n",
      "139 1.0491175651550293\n",
      "140 1.0489897727966309\n",
      "141 1.048863172531128\n",
      "142 1.048736572265625\n",
      "143 1.0486098527908325\n",
      "144 1.0484837293624878\n",
      "145 1.048356533050537\n",
      "146 1.0482300519943237\n",
      "147 1.048103928565979\n",
      "148 1.0479767322540283\n",
      "149 1.047850489616394\n",
      "150 1.047724723815918\n",
      "151 1.0475977659225464\n",
      "152 1.0474708080291748\n",
      "153 1.0473437309265137\n",
      "154 1.0472182035446167\n",
      "155 1.0470916032791138\n",
      "156 1.04696524143219\n",
      "157 1.046838641166687\n",
      "158 1.0467122793197632\n",
      "159 1.046586275100708\n",
      "160 1.046459436416626\n",
      "161 1.0463335514068604\n",
      "162 1.046207308769226\n",
      "163 1.04608154296875\n",
      "164 1.045954942703247\n",
      "165 1.045828104019165\n",
      "166 1.0457029342651367\n",
      "167 1.0455769300460815\n",
      "168 1.0454514026641846\n",
      "169 1.0453251600265503\n",
      "170 1.0451991558074951\n",
      "171 1.0450730323791504\n",
      "172 1.0449471473693848\n",
      "173 1.044820785522461\n",
      "174 1.0446946620941162\n",
      "175 1.0445692539215088\n",
      "176 1.0444433689117432\n",
      "177 1.044317364692688\n",
      "178 1.0441919565200806\n",
      "179 1.0440670251846313\n",
      "180 1.0439412593841553\n",
      "181 1.0438162088394165\n",
      "182 1.0436915159225464\n",
      "183 1.043566107749939\n",
      "184 1.0434412956237793\n",
      "185 1.0433158874511719\n",
      "186 1.043190836906433\n",
      "187 1.043067216873169\n",
      "188 1.0429418087005615\n",
      "189 1.0428162813186646\n",
      "190 1.0426918268203735\n",
      "191 1.0425671339035034\n",
      "192 1.0424429178237915\n",
      "193 1.042317271232605\n",
      "194 1.0421923398971558\n",
      "195 1.0420676469802856\n",
      "196 1.0419433116912842\n",
      "197 1.0418190956115723\n",
      "198 1.0416944026947021\n",
      "199 1.0415695905685425\n",
      "200 1.0414448976516724\n",
      "201 1.0413200855255127\n",
      "202 1.0411955118179321\n",
      "203 1.0410723686218262\n",
      "204 1.0409467220306396\n",
      "205 1.0408214330673218\n",
      "206 1.0406981706619263\n",
      "207 1.0405733585357666\n",
      "208 1.0404490232467651\n",
      "209 1.0403244495391846\n",
      "210 1.0402004718780518\n",
      "211 1.0400760173797607\n",
      "212 1.0399510860443115\n",
      "213 1.0398271083831787\n",
      "214 1.0397025346755981\n",
      "215 1.0395785570144653\n",
      "216 1.0394542217254639\n",
      "217 1.0393292903900146\n",
      "218 1.0392053127288818\n",
      "219 1.039081335067749\n",
      "220 1.0389569997787476\n",
      "221 1.038832664489746\n",
      "222 1.0387089252471924\n",
      "223 1.0385847091674805\n",
      "224 1.0384622812271118\n",
      "225 1.038336992263794\n",
      "226 1.0382131338119507\n",
      "227 1.038089394569397\n",
      "228 1.0379645824432373\n",
      "229 1.0378416776657104\n",
      "230 1.0377169847488403\n",
      "231 1.0375926494598389\n",
      "232 1.0374689102172852\n",
      "233 1.0373438596725464\n",
      "234 1.0372211933135986\n",
      "235 1.0370975732803345\n",
      "236 1.0369731187820435\n",
      "237 1.0368496179580688\n",
      "238 1.0367257595062256\n",
      "239 1.0366021394729614\n",
      "240 1.03647780418396\n",
      "241 1.0363547801971436\n",
      "242 1.036231279373169\n",
      "243 1.0361078977584839\n",
      "244 1.035983681678772\n",
      "245 1.035860300064087\n",
      "246 1.0357370376586914\n",
      "247 1.035612940788269\n",
      "248 1.0354902744293213\n",
      "249 1.0353670120239258\n",
      "250 1.0352436304092407\n",
      "251 1.0351200103759766\n",
      "252 1.034996747970581\n",
      "253 1.0348726511001587\n",
      "254 1.0347492694854736\n",
      "255 1.0346262454986572\n",
      "256 1.03450345993042\n",
      "257 1.034380555152893\n",
      "258 1.0342570543289185\n",
      "259 1.0341339111328125\n",
      "260 1.0340101718902588\n",
      "261 1.0338876247406006\n",
      "262 1.0337644815444946\n",
      "263 1.0336416959762573\n",
      "264 1.0335184335708618\n",
      "265 1.0333962440490723\n",
      "266 1.0332715511322021\n",
      "267 1.033149242401123\n",
      "268 1.0330263376235962\n",
      "269 1.0329036712646484\n",
      "270 1.0327812433242798\n",
      "271 1.0326578617095947\n",
      "272 1.032535433769226\n",
      "273 1.0324122905731201\n",
      "274 1.032289743423462\n",
      "275 1.0321671962738037\n",
      "276 1.032043695449829\n",
      "277 1.0319206714630127\n",
      "278 1.0317983627319336\n",
      "279 1.0316758155822754\n",
      "280 1.0315525531768799\n",
      "281 1.0314290523529053\n",
      "282 1.0313069820404053\n",
      "283 1.0311843156814575\n",
      "284 1.031062126159668\n",
      "285 1.0309388637542725\n",
      "286 1.0308167934417725\n",
      "287 1.0306941270828247\n",
      "288 1.030571699142456\n",
      "289 1.030449390411377\n",
      "290 1.0303270816802979\n",
      "291 1.0302046537399292\n",
      "292 1.0300824642181396\n",
      "293 1.0299607515335083\n",
      "294 1.0298385620117188\n",
      "295 1.029717206954956\n",
      "296 1.0295941829681396\n",
      "297 1.0294723510742188\n",
      "298 1.029350757598877\n",
      "299 1.029228687286377\n",
      "300 1.0291064977645874\n",
      "301 1.028985619544983\n",
      "302 1.0288642644882202\n",
      "303 1.0287415981292725\n",
      "304 1.0286201238632202\n",
      "305 1.0284982919692993\n",
      "306 1.0283762216567993\n",
      "307 1.028254747390747\n",
      "308 1.0281333923339844\n",
      "309 1.0280117988586426\n",
      "310 1.0278899669647217\n",
      "311 1.0277684926986694\n",
      "312 1.0276480913162231\n",
      "313 1.027525782585144\n",
      "314 1.027404546737671\n",
      "315 1.0272825956344604\n",
      "316 1.0271615982055664\n",
      "317 1.0270397663116455\n",
      "318 1.0269193649291992\n",
      "319 1.0267969369888306\n",
      "320 1.0266762971878052\n",
      "321 1.0265557765960693\n",
      "322 1.0264350175857544\n",
      "323 1.0263129472732544\n",
      "324 1.0261924266815186\n",
      "325 1.0260709524154663\n",
      "326 1.025949239730835\n",
      "327 1.02582848072052\n",
      "328 1.0257079601287842\n",
      "329 1.0255868434906006\n",
      "330 1.025465965270996\n",
      "331 1.0253455638885498\n",
      "332 1.0252248048782349\n",
      "333 1.0251038074493408\n",
      "334 1.0249831676483154\n",
      "335 1.02486252784729\n",
      "336 1.024741768836975\n",
      "337 1.024621844291687\n",
      "338 1.024501085281372\n",
      "339 1.0243806838989258\n",
      "340 1.024260401725769\n",
      "341 1.0241398811340332\n",
      "342 1.0240187644958496\n",
      "343 1.023898720741272\n",
      "344 1.0237784385681152\n",
      "345 1.0236573219299316\n",
      "346 1.0235381126403809\n",
      "347 1.0234172344207764\n",
      "348 1.0232967138290405\n",
      "349 1.023176908493042\n",
      "350 1.0230562686920166\n",
      "351 1.0229365825653076\n",
      "352 1.0228163003921509\n",
      "353 1.0226961374282837\n",
      "354 1.022576093673706\n",
      "355 1.0224558115005493\n",
      "356 1.0223352909088135\n",
      "357 1.0222150087356567\n",
      "358 1.022094964981079\n",
      "359 1.0219746828079224\n",
      "360 1.0218548774719238\n",
      "361 1.0217348337173462\n",
      "362 1.0216145515441895\n",
      "363 1.02149498462677\n",
      "364 1.0213749408721924\n",
      "365 1.0212554931640625\n",
      "366 1.021134853363037\n",
      "367 1.0210155248641968\n",
      "368 1.0208948850631714\n",
      "369 1.0207750797271729\n",
      "370 1.0206549167633057\n",
      "371 1.0205355882644653\n",
      "372 1.0204155445098877\n",
      "373 1.0202962160110474\n",
      "374 1.0201761722564697\n",
      "375 1.0200567245483398\n",
      "376 1.0199365615844727\n",
      "377 1.0198169946670532\n",
      "378 1.019696831703186\n",
      "379 1.0195777416229248\n",
      "380 1.0194575786590576\n",
      "381 1.0193383693695068\n",
      "382 1.0192186832427979\n",
      "383 1.0190982818603516\n",
      "384 1.0189793109893799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 1.0188593864440918\n",
      "386 1.018740177154541\n",
      "387 1.018620252609253\n",
      "388 1.0185012817382812\n",
      "389 1.0183818340301514\n",
      "390 1.018262505531311\n",
      "391 1.0181429386138916\n",
      "392 1.0180236101150513\n",
      "393 1.0179040431976318\n",
      "394 1.0177847146987915\n",
      "395 1.0176644325256348\n",
      "396 1.0175449848175049\n",
      "397 1.017426609992981\n",
      "398 1.017307996749878\n",
      "399 1.017187237739563\n",
      "400 1.0170689821243286\n",
      "401 1.016948938369751\n",
      "402 1.0168300867080688\n",
      "403 1.0167112350463867\n",
      "404 1.016592025756836\n",
      "405 1.0164722204208374\n",
      "406 1.0163538455963135\n",
      "407 1.0162348747253418\n",
      "408 1.016115427017212\n",
      "409 1.0159969329833984\n",
      "410 1.0158777236938477\n",
      "411 1.015760064125061\n",
      "412 1.0156406164169312\n",
      "413 1.0155225992202759\n",
      "414 1.0154035091400146\n",
      "415 1.0152848958969116\n",
      "416 1.0151667594909668\n",
      "417 1.0150482654571533\n",
      "418 1.014930009841919\n",
      "419 1.0148112773895264\n",
      "420 1.0146929025650024\n",
      "421 1.014574408531189\n",
      "422 1.0144556760787964\n",
      "423 1.0143373012542725\n",
      "424 1.0142195224761963\n",
      "425 1.0141007900238037\n",
      "426 1.0139825344085693\n",
      "427 1.0138651132583618\n",
      "428 1.0137463808059692\n",
      "429 1.0136284828186035\n",
      "430 1.013508915901184\n",
      "431 1.0133920907974243\n",
      "432 1.0132732391357422\n",
      "433 1.01315438747406\n",
      "434 1.0130364894866943\n",
      "435 1.0129177570343018\n",
      "436 1.0128004550933838\n",
      "437 1.0126826763153076\n",
      "438 1.0125645399093628\n",
      "439 1.0124456882476807\n",
      "440 1.0123283863067627\n",
      "441 1.0122106075286865\n",
      "442 1.0120930671691895\n",
      "443 1.011974811553955\n",
      "444 1.011857032775879\n",
      "445 1.0117390155792236\n",
      "446 1.011621117591858\n",
      "447 1.0115031003952026\n",
      "448 1.0113856792449951\n",
      "449 1.0112676620483398\n",
      "450 1.0111504793167114\n",
      "451 1.0110324621200562\n",
      "452 1.010914921760559\n",
      "453 1.0107972621917725\n",
      "454 1.0106801986694336\n",
      "455 1.0105619430541992\n",
      "456 1.0104445219039917\n",
      "457 1.010326862335205\n",
      "458 1.0102094411849976\n",
      "459 1.0100916624069214\n",
      "460 1.0099742412567139\n",
      "461 1.0098570585250854\n",
      "462 1.0097393989562988\n",
      "463 1.00962233543396\n",
      "464 1.0095051527023315\n",
      "465 1.0093871355056763\n",
      "466 1.0092698335647583\n",
      "467 1.0091526508331299\n",
      "468 1.0090352296829224\n",
      "469 1.0089179277420044\n",
      "470 1.0088005065917969\n",
      "471 1.0086835622787476\n",
      "472 1.008565902709961\n",
      "473 1.0084490776062012\n",
      "474 1.008331060409546\n",
      "475 1.0082143545150757\n",
      "476 1.0080969333648682\n",
      "477 1.0079801082611084\n",
      "478 1.0078626871109009\n",
      "479 1.0077455043792725\n",
      "480 1.0076285600662231\n",
      "481 1.007512092590332\n",
      "482 1.007394790649414\n",
      "483 1.007277488708496\n",
      "484 1.0071605443954468\n",
      "485 1.0070433616638184\n",
      "486 1.0069265365600586\n",
      "487 1.0068097114562988\n",
      "488 1.0066931247711182\n",
      "489 1.0065758228302002\n",
      "490 1.0064582824707031\n",
      "491 1.006341576576233\n",
      "492 1.0062243938446045\n",
      "493 1.0061081647872925\n",
      "494 1.0059913396835327\n",
      "495 1.0058746337890625\n",
      "496 1.0057575702667236\n",
      "497 1.0056414604187012\n",
      "498 1.0055240392684937\n",
      "499 1.005406379699707\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = forward_fn(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    forward_fn.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with tc.no_grad():\n",
    "        for param in forward_fn.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using __torch.optim__ to simplify the result, the training above is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0052906274795532\n",
      "1 0.9796911478042603\n",
      "2 0.9547818899154663\n",
      "3 0.9306782484054565\n",
      "4 0.9072775840759277\n",
      "5 0.8845944404602051\n",
      "6 0.8626068830490112\n",
      "7 0.8413504362106323\n",
      "8 0.8205755352973938\n",
      "9 0.8003190159797668\n",
      "10 0.7806328535079956\n",
      "11 0.7614113092422485\n",
      "12 0.7427704930305481\n",
      "13 0.7247215509414673\n",
      "14 0.7072141766548157\n",
      "15 0.6902074217796326\n",
      "16 0.6735973954200745\n",
      "17 0.6574391722679138\n",
      "18 0.6417974233627319\n",
      "19 0.626603364944458\n",
      "20 0.6118294596672058\n",
      "21 0.5973835587501526\n",
      "22 0.5833657383918762\n",
      "23 0.5696368217468262\n",
      "24 0.5561829805374146\n",
      "25 0.543042004108429\n",
      "26 0.5301986932754517\n",
      "27 0.5176398158073425\n",
      "28 0.5053831934928894\n",
      "29 0.49341195821762085\n",
      "30 0.4816928803920746\n",
      "31 0.4702206254005432\n",
      "32 0.4590657651424408\n",
      "33 0.4481775760650635\n",
      "34 0.4375523626804352\n",
      "35 0.4271828234195709\n",
      "36 0.41702190041542053\n",
      "37 0.4070528447628021\n",
      "38 0.39730626344680786\n",
      "39 0.3877413868904114\n",
      "40 0.3783687949180603\n",
      "41 0.3692431151866913\n",
      "42 0.3603104054927826\n",
      "43 0.3515719175338745\n",
      "44 0.3430081605911255\n",
      "45 0.33460262417793274\n",
      "46 0.32637375593185425\n",
      "47 0.3183160424232483\n",
      "48 0.31043824553489685\n",
      "49 0.3027210831642151\n",
      "50 0.2951406240463257\n",
      "51 0.2877103388309479\n",
      "52 0.28041598200798035\n",
      "53 0.27325403690338135\n",
      "54 0.2661997973918915\n",
      "55 0.25928062200546265\n",
      "56 0.25250381231307983\n",
      "57 0.2458379566669464\n",
      "58 0.2393161505460739\n",
      "59 0.23294281959533691\n",
      "60 0.226711243391037\n",
      "61 0.22059956192970276\n",
      "62 0.21461239457130432\n",
      "63 0.20876209437847137\n",
      "64 0.2030399590730667\n",
      "65 0.19743309915065765\n",
      "66 0.19193902611732483\n",
      "67 0.1865769475698471\n",
      "68 0.181339293718338\n",
      "69 0.1762116551399231\n",
      "70 0.17119385302066803\n",
      "71 0.16628865897655487\n",
      "72 0.16147799789905548\n",
      "73 0.1567770391702652\n",
      "74 0.15217086672782898\n",
      "75 0.14767110347747803\n",
      "76 0.1432785838842392\n",
      "77 0.1389928162097931\n",
      "78 0.134808748960495\n",
      "79 0.13072097301483154\n",
      "80 0.12672850489616394\n",
      "81 0.12282781302928925\n",
      "82 0.11902675777673721\n",
      "83 0.11532384157180786\n",
      "84 0.11171535402536392\n",
      "85 0.10819216817617416\n",
      "86 0.10475645959377289\n",
      "87 0.10142134130001068\n",
      "88 0.09817163646221161\n",
      "89 0.09499948471784592\n",
      "90 0.09191198647022247\n",
      "91 0.0889064148068428\n",
      "92 0.08597838133573532\n",
      "93 0.08313403278589249\n",
      "94 0.08036795258522034\n",
      "95 0.07767479121685028\n",
      "96 0.07505451142787933\n",
      "97 0.07250557839870453\n",
      "98 0.07002777606248856\n",
      "99 0.06761885434389114\n",
      "100 0.06528232246637344\n",
      "101 0.06301401555538177\n",
      "102 0.060816146433353424\n",
      "103 0.058679234236478806\n",
      "104 0.05660378187894821\n",
      "105 0.05459364503622055\n",
      "106 0.05264652520418167\n",
      "107 0.050754398107528687\n",
      "108 0.04891885817050934\n",
      "109 0.04713796451687813\n",
      "110 0.04541337117552757\n",
      "111 0.043740879744291306\n",
      "112 0.04212217405438423\n",
      "113 0.04055403918027878\n",
      "114 0.03903784602880478\n",
      "115 0.03756926208734512\n",
      "116 0.03614967316389084\n",
      "117 0.03477553278207779\n",
      "118 0.0334460586309433\n",
      "119 0.03216046839952469\n",
      "120 0.030919160693883896\n",
      "121 0.029718389734625816\n",
      "122 0.02855772338807583\n",
      "123 0.027436867356300354\n",
      "124 0.026353999972343445\n",
      "125 0.025308486074209213\n",
      "126 0.024299252778291702\n",
      "127 0.023323455825448036\n",
      "128 0.022381309419870377\n",
      "129 0.02147240936756134\n",
      "130 0.020596016198396683\n",
      "131 0.019749609753489494\n",
      "132 0.0189345795661211\n",
      "133 0.018148211762309074\n",
      "134 0.017391012981534004\n",
      "135 0.01666306145489216\n",
      "136 0.0159614197909832\n",
      "137 0.015285293571650982\n",
      "138 0.014634834602475166\n",
      "139 0.014008330181241035\n",
      "140 0.013405954465270042\n",
      "141 0.012827309779822826\n",
      "142 0.0122710932046175\n",
      "143 0.01173656340688467\n",
      "144 0.011223861016333103\n",
      "145 0.010731125250458717\n",
      "146 0.010258423164486885\n",
      "147 0.009805409237742424\n",
      "148 0.009370377287268639\n",
      "149 0.00895274430513382\n",
      "150 0.008552221581339836\n",
      "151 0.008167607709765434\n",
      "152 0.007798950187861919\n",
      "153 0.007445089519023895\n",
      "154 0.007105696015059948\n",
      "155 0.006780236028134823\n",
      "156 0.006468825973570347\n",
      "157 0.006170510780066252\n",
      "158 0.005885194055736065\n",
      "159 0.005611945874989033\n",
      "160 0.005351004656404257\n",
      "161 0.0051020728424191475\n",
      "162 0.00486410316079855\n",
      "163 0.004636503290385008\n",
      "164 0.004419119097292423\n",
      "165 0.004211182240396738\n",
      "166 0.004012662917375565\n",
      "167 0.0038232363294810057\n",
      "168 0.003642220515757799\n",
      "169 0.003469449933618307\n",
      "170 0.0033044316805899143\n",
      "171 0.0031468074303120375\n",
      "172 0.0029964898712933064\n",
      "173 0.002853166777640581\n",
      "174 0.002716371789574623\n",
      "175 0.0025859656743705273\n",
      "176 0.0024616913869976997\n",
      "177 0.002343178493902087\n",
      "178 0.002230255398899317\n",
      "179 0.0021225656382739544\n",
      "180 0.002020058920606971\n",
      "181 0.0019224205752834678\n",
      "182 0.001829377026297152\n",
      "183 0.0017407126724720001\n",
      "184 0.0016563903773203492\n",
      "185 0.0015761168906465173\n",
      "186 0.0014996242243796587\n",
      "187 0.0014267530059441924\n",
      "188 0.0013574168551713228\n",
      "189 0.0012914256658405066\n",
      "190 0.0012285984121263027\n",
      "191 0.0011688058730214834\n",
      "192 0.0011118825059384108\n",
      "193 0.0010577578796073794\n",
      "194 0.0010062129003927112\n",
      "195 0.0009571553091518581\n",
      "196 0.000910424452740699\n",
      "197 0.0008659712038934231\n",
      "198 0.0008236699504777789\n",
      "199 0.000783408060669899\n",
      "200 0.0007451307610608637\n",
      "201 0.0007086881087161601\n",
      "202 0.00067402224522084\n",
      "203 0.0006410242058336735\n",
      "204 0.0006095908465795219\n",
      "205 0.0005796791519969702\n",
      "206 0.000551232136785984\n",
      "207 0.000524142466019839\n",
      "208 0.0004983891849406064\n",
      "209 0.00047385701327584684\n",
      "210 0.00045051699271425605\n",
      "211 0.00042830914026126266\n",
      "212 0.0004072003939654678\n",
      "213 0.0003870979999192059\n",
      "214 0.0003679878718685359\n",
      "215 0.00034979876363649964\n",
      "216 0.00033249816624447703\n",
      "217 0.0003160382038913667\n",
      "218 0.00030040316050872207\n",
      "219 0.00028551401919685304\n",
      "220 0.0002713687135837972\n",
      "221 0.00025791270309127867\n",
      "222 0.0002451131003908813\n",
      "223 0.00023295159917324781\n",
      "224 0.0002213787956861779\n",
      "225 0.00021037673286627978\n",
      "226 0.00019992320449091494\n",
      "227 0.00018999565509147942\n",
      "228 0.00018055483815260231\n",
      "229 0.00017157594265881926\n",
      "230 0.0001630485348869115\n",
      "231 0.00015492862439714372\n",
      "232 0.00014722062041983008\n",
      "233 0.00013987986312713474\n",
      "234 0.0001329082588199526\n",
      "235 0.00012627933756448328\n",
      "236 0.00011997984256595373\n",
      "237 0.0001139850210165605\n",
      "238 0.00010828940139617771\n",
      "239 0.0001028740662150085\n",
      "240 9.772645717021078e-05\n",
      "241 9.283157851314172e-05\n",
      "242 8.818108472041786e-05\n",
      "243 8.375669858651236e-05\n",
      "244 7.95570740592666e-05\n",
      "245 7.556330820079893e-05\n",
      "246 7.176741200964898e-05\n",
      "247 6.816078530391678e-05\n",
      "248 6.473115354310721e-05\n",
      "249 6.147346721263602e-05\n",
      "250 5.8375597291160375e-05\n",
      "251 5.543312363442965e-05\n",
      "252 5.263651837594807e-05\n",
      "253 4.997828000341542e-05\n",
      "254 4.7454730520257726e-05\n",
      "255 4.5055818191030994e-05\n",
      "256 4.277677362551913e-05\n",
      "257 4.06120961997658e-05\n",
      "258 3.855617978842929e-05\n",
      "259 3.6603269109036773e-05\n",
      "260 3.4745880839182064e-05\n",
      "261 3.298245428595692e-05\n",
      "262 3.13108139380347e-05\n",
      "263 2.9725586500717327e-05\n",
      "264 2.8220170861459337e-05\n",
      "265 2.679075078049209e-05\n",
      "266 2.5433360860915855e-05\n",
      "267 2.4143791961250827e-05\n",
      "268 2.2919761249795556e-05\n",
      "269 2.1756823116447777e-05\n",
      "270 2.0652994862757623e-05\n",
      "271 1.96045857592253e-05\n",
      "272 1.860947122622747e-05\n",
      "273 1.7664377082837746e-05\n",
      "274 1.6766978660598397e-05\n",
      "275 1.5915178664727136e-05\n",
      "276 1.5106534192455001e-05\n",
      "277 1.43384850161965e-05\n",
      "278 1.3609552297566552e-05\n",
      "279 1.2917495041619986e-05\n",
      "280 1.2260288713150658e-05\n",
      "281 1.1636702765827067e-05\n",
      "282 1.1044638085877523e-05\n",
      "283 1.0482403922651429e-05\n",
      "284 9.948988918040413e-06\n",
      "285 9.442551345273387e-06\n",
      "286 8.961649655248038e-06\n",
      "287 8.50527885631891e-06\n",
      "288 8.072058335528709e-06\n",
      "289 7.660850315005518e-06\n",
      "290 7.270515197888017e-06\n",
      "291 6.900028438394656e-06\n",
      "292 6.548286364704836e-06\n",
      "293 6.214458608155837e-06\n",
      "294 5.8976811487809755e-06\n",
      "295 5.596979463007301e-06\n",
      "296 5.311628683557501e-06\n",
      "297 5.040694304625504e-06\n",
      "298 4.78359197586542e-06\n",
      "299 4.539579549600603e-06\n",
      "300 4.308000825403724e-06\n",
      "301 4.0881664062908385e-06\n",
      "302 3.879611540469341e-06\n",
      "303 3.681760063045658e-06\n",
      "304 3.4938043427246157e-06\n",
      "305 3.315472667964059e-06\n",
      "306 3.1462507195101352e-06\n",
      "307 2.9856332730560098e-06\n",
      "308 2.8331801331660245e-06\n",
      "309 2.6885195438808296e-06\n",
      "310 2.551196985223214e-06\n",
      "311 2.420960072413436e-06\n",
      "312 2.2972449187363964e-06\n",
      "313 2.179874172725249e-06\n",
      "314 2.068482444883557e-06\n",
      "315 1.962748001460568e-06\n",
      "316 1.8624259610078298e-06\n",
      "317 1.7672111880528973e-06\n",
      "318 1.6768648265497177e-06\n",
      "319 1.5910925412754295e-06\n",
      "320 1.5096991319296649e-06\n",
      "321 1.4324220956041245e-06\n",
      "322 1.359096586384112e-06\n",
      "323 1.2895385452793562e-06\n",
      "324 1.2234650057507679e-06\n",
      "325 1.1607978649408324e-06\n",
      "326 1.1013227094736067e-06\n",
      "327 1.0448866305523552e-06\n",
      "328 9.913231906466535e-07\n",
      "329 9.404475349583663e-07\n",
      "330 8.922167467062536e-07\n",
      "331 8.464148777420633e-07\n",
      "332 8.029531386455346e-07\n",
      "333 7.617288133587863e-07\n",
      "334 7.225915510389314e-07\n",
      "335 6.854800744804379e-07\n",
      "336 6.502164069388527e-07\n",
      "337 6.167592800920829e-07\n",
      "338 5.850245656802144e-07\n",
      "339 5.548983494918502e-07\n",
      "340 5.263152615953004e-07\n",
      "341 4.991871378479118e-07\n",
      "342 4.734571064091142e-07\n",
      "343 4.4902762397214246e-07\n",
      "344 4.258498904619046e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 4.0386194655184227e-07\n",
      "346 3.8299353377624357e-07\n",
      "347 3.631950562521524e-07\n",
      "348 3.4440910212651943e-07\n",
      "349 3.265858481427131e-07\n",
      "350 3.096645571076806e-07\n",
      "351 2.9363314979491406e-07\n",
      "352 2.78393201824656e-07\n",
      "353 2.639428657857934e-07\n",
      "354 2.502384006675129e-07\n",
      "355 2.3723104902728664e-07\n",
      "356 2.24904624701594e-07\n",
      "357 2.1320367693533626e-07\n",
      "358 2.020963165705325e-07\n",
      "359 1.9156041730639117e-07\n",
      "360 1.8157513181904505e-07\n",
      "361 1.7209465852374706e-07\n",
      "362 1.6310953299125686e-07\n",
      "363 1.545854075857278e-07\n",
      "364 1.4649080526396574e-07\n",
      "365 1.3882382177143882e-07\n",
      "366 1.3155131739495118e-07\n",
      "367 1.2464536780498747e-07\n",
      "368 1.1810294608949334e-07\n",
      "369 1.118939039201905e-07\n",
      "370 1.0601259248232964e-07\n",
      "371 1.0042350595540483e-07\n",
      "372 9.513381371561991e-08\n",
      "373 9.011549906290384e-08\n",
      "374 8.536104445511228e-08\n",
      "375 8.084916913730922e-08\n",
      "376 7.657357059542846e-08\n",
      "377 7.25156823477846e-08\n",
      "378 6.867001189903021e-08\n",
      "379 6.502632743377035e-08\n",
      "380 6.157085863378597e-08\n",
      "381 5.8300162919522336e-08\n",
      "382 5.519601131709351e-08\n",
      "383 5.225517796247914e-08\n",
      "384 4.94648801918629e-08\n",
      "385 4.682509313624905e-08\n",
      "386 4.432213529526052e-08\n",
      "387 4.194735225837576e-08\n",
      "388 3.970483675175274e-08\n",
      "389 3.7572380762185276e-08\n",
      "390 3.555784999775824e-08\n",
      "391 3.364517908721609e-08\n",
      "392 3.183296115594203e-08\n",
      "393 3.0118162186454356e-08\n",
      "394 2.8493635895188163e-08\n",
      "395 2.695434986321743e-08\n",
      "396 2.5496666111735067e-08\n",
      "397 2.4118088859381714e-08\n",
      "398 2.2810031197195713e-08\n",
      "399 2.1572905239963802e-08\n",
      "400 2.040034985384409e-08\n",
      "401 1.9290151698214686e-08\n",
      "402 1.824123430083091e-08\n",
      "403 1.7245149308564578e-08\n",
      "404 1.6303383532090265e-08\n",
      "405 1.5414268972335776e-08\n",
      "406 1.4569893735938422e-08\n",
      "407 1.3771021656339144e-08\n",
      "408 1.301369412232134e-08\n",
      "409 1.2300179541568923e-08\n",
      "410 1.1623389362114267e-08\n",
      "411 1.0983263187824832e-08\n",
      "412 1.0378505166386276e-08\n",
      "413 9.804719702799503e-09\n",
      "414 9.263269262760332e-09\n",
      "415 8.749685420639253e-09\n",
      "416 8.266021644942612e-09\n",
      "417 7.805953217143724e-09\n",
      "418 7.371284027613001e-09\n",
      "419 6.961292431384436e-09\n",
      "420 6.5734893084368196e-09\n",
      "421 6.206349212334317e-09\n",
      "422 5.859451590595199e-09\n",
      "423 5.531312297080149e-09\n",
      "424 5.222083654388143e-09\n",
      "425 4.927731112047695e-09\n",
      "426 4.6522741214971575e-09\n",
      "427 4.38958291937297e-09\n",
      "428 4.143284382251977e-09\n",
      "429 3.909199186580281e-09\n",
      "430 3.688364724752091e-09\n",
      "431 3.4791873826378605e-09\n",
      "432 3.2819269524253514e-09\n",
      "433 3.0964888431128657e-09\n",
      "434 2.9203139906286424e-09\n",
      "435 2.754198202836733e-09\n",
      "436 2.5977786588526897e-09\n",
      "437 2.449161318196502e-09\n",
      "438 2.3089792300368117e-09\n",
      "439 2.1770190095082853e-09\n",
      "440 2.0523054367060922e-09\n",
      "441 1.934330473574164e-09\n",
      "442 1.8229560083682372e-09\n",
      "443 1.7177825828440518e-09\n",
      "444 1.6192889251698261e-09\n",
      "445 1.5254398855191198e-09\n",
      "446 1.4378603863107742e-09\n",
      "447 1.3541159304963912e-09\n",
      "448 1.2756172784733621e-09\n",
      "449 1.2017198347535896e-09\n",
      "450 1.131570392942649e-09\n",
      "451 1.065656340948351e-09\n",
      "452 1.0034627573318744e-09\n",
      "453 9.4509067238846e-10\n",
      "454 8.897403924734704e-10\n",
      "455 8.376190296921493e-10\n",
      "456 7.881902908124516e-10\n",
      "457 7.417909619888974e-10\n",
      "458 6.981151212670511e-10\n",
      "459 6.567099641863194e-10\n",
      "460 6.182647727115409e-10\n",
      "461 5.819758008840381e-10\n",
      "462 5.470635611182217e-10\n",
      "463 5.146063020156078e-10\n",
      "464 4.840294831609526e-10\n",
      "465 4.5530254566550354e-10\n",
      "466 4.281880128242932e-10\n",
      "467 4.022577826390261e-10\n",
      "468 3.7861419555085263e-10\n",
      "469 3.5592076508272896e-10\n",
      "470 3.3460023640685677e-10\n",
      "471 3.144032256763296e-10\n",
      "472 2.9528474110307457e-10\n",
      "473 2.776711638396989e-10\n",
      "474 2.606948545924581e-10\n",
      "475 2.44901154911048e-10\n",
      "476 2.3026816564630792e-10\n",
      "477 2.1621671120186647e-10\n",
      "478 2.0288601354501168e-10\n",
      "479 1.9062276757075836e-10\n",
      "480 1.7893261872181654e-10\n",
      "481 1.6806138425362604e-10\n",
      "482 1.5772327888186055e-10\n",
      "483 1.4828982486392306e-10\n",
      "484 1.390824955205261e-10\n",
      "485 1.3049764047146084e-10\n",
      "486 1.2246188785258738e-10\n",
      "487 1.1486193940424272e-10\n",
      "488 1.0771254721486656e-10\n",
      "489 1.0118108434431505e-10\n",
      "490 9.497156533422313e-11\n",
      "491 8.901977766040403e-11\n",
      "492 8.348480240449874e-11\n",
      "493 7.825973202812975e-11\n",
      "494 7.348364972070698e-11\n",
      "495 6.887887482598387e-11\n",
      "496 6.465211699335782e-11\n",
      "497 6.05713881829395e-11\n",
      "498 5.6718164886948585e-11\n",
      "499 5.321353427345521e-11\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = tc.optim.Adam(forward_fn.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = forward_fn(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training  CNN on MNIST\n",
    "Step 0: load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_data = tv.datasets.MNIST(root='.data/mnist', train=True,\n",
    "                                            transform=tv.transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    test_data = tv.datasets.MNIST(root='.data/mnist', train=False,\n",
    "                                            transform=tv.transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "def imshow(instance, label):\n",
    "    plt.imshow(instance.reshape(28,28), cmap='gray')\n",
    "    plt.title('%i' % label, fontsize = 20)\n",
    "    plt.show()\n",
    "\n",
    "train_data, test_data = load_data()\n",
    "X_test = test_data.test_data.reshape(10000,1,28,28)\n",
    "X_test = tc.tensor(X_test,dtype=tc.float) / 255\n",
    "y_test = test_data.test_labels\n",
    "import torch.utils.data as Data\n",
    "train_batch = Data.DataLoader(dataset=train_data, batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(X_test[1], y_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: define forward function\n",
    "\n",
    "    layer 1: Convolutional + Relu + Maxpooling\n",
    "    layer 2: Convolutional + Relu + Maxpooling\n",
    "    layer 3: linear\n",
    "    \n",
    "reference:\n",
    "Conv2d:https://pytorch.org/docs/stable/nn.html?highlight=conv#torch.nn.functional.conv2d\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tc.nn\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1_conv = nn.Sequential(  # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height\n",
    "                out_channels=16,    # n_filters\n",
    "                kernel_size=5,      # filter size\n",
    "                stride=1,           # filter movement/step\n",
    "                padding=2,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1\n",
    "            ),      # output shape (16, 28, 28)\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 14, 14)\n",
    "        )\n",
    "        self.layer2_conv = nn.Sequential(  # input shape (1, 28, 28)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.MaxPool2d(2),  # output shape (32, 7, 7)\n",
    "        )\n",
    "        self.layer3_linear = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1_conv(x)\n",
    "        x = self.layer2_conv(x)\n",
    "        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)\n",
    "        output = self.layer3_linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: define loss\n",
    "\n",
    "cross entropy:\n",
    "    https://pytorch.org/docs/stable/nn.html?highlight=cross%20entropy#torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_mnist = CNN()\n",
    "optimizer = tc.optim.Adam(cnn_mnist.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, several consideration are needed:\n",
    "1. how many round? how big is the batch each time?\n",
    "```\n",
    "    for each_round in epoch:\n",
    "        for each_example in  example:\n",
    "```\n",
    "2. what about testing set?\n",
    "3. what about logging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henryliu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 |Step: 0 |train loss:0.2894 |test accuracy:0.9457\n",
      "Epoch: 0 |Step: 100 |train loss:0.1667 |test accuracy:0.9614\n",
      "Epoch: 0 |Step: 200 |train loss:0.2264 |test accuracy:0.9697\n",
      "Epoch: 0 |Step: 300 |train loss:0.0412 |test accuracy:0.9761\n",
      "Epoch: 0 |Step: 400 |train loss:0.0852 |test accuracy:0.9791\n",
      "Epoch: 0 |Step: 500 |train loss:0.0337 |test accuracy:0.9803\n",
      "Epoch: 1 |Step: 0 |train loss:0.0611 |test accuracy:0.9810\n",
      "Epoch: 1 |Step: 100 |train loss:0.0260 |test accuracy:0.9812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-15053903cd8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for step, (x, y) in enumerate(train_batch):\n",
    "        output = cnn_mnist(x)\n",
    "        loss = loss_func(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        if step % 100 == 0:\n",
    "            outputs = cnn_mnist(X_test)\n",
    "            _, predicted = tc.max(outputs.data, 1)\n",
    "            total = y_test.size(0)\n",
    "            correct = (predicted == y_test).sum().item()\n",
    "            accuracy = correct/total\n",
    "            print('Epoch:', epoch, '|Step:', step,\n",
    "                  '|train loss:%.4f' % loss.data[0], '|test accuracy:%.4f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
